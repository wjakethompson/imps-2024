---
title: Evaluating model fit
subtitle: For Bayesian diagnostic classification models
author: W. Jake Thompson, Ph.D.
format:
  measr-slides-revealjs:
    progress: false
knitr:
  opts_chunk: 
    comment: "#>"
    fig.width: 7
    fig.asp: 0.618
    fig.align: "center"
code-link: true
preload-iframes: true
code-annotations: select
filters:
  - lua/output-line-highlight.lua
---

## Who am I?

```{r setup}
library(tidyverse)
library(countdown)
library(ggmeasr)
library(knitr)
library(measr)
library(here)
library(posterior)

set_theme(plot_margin = margin(5, 0, 0, 0))
```

:::{.columns}
:::{.column width="50%"}
W. Jake Thompson, Ph.D.

* Assistant Director of Psychometrics
  * [ATLAS](https://atlas.ku.edu) | University of Kansas

* Research: Applications of diagnostic psychometric models
  * Lead psychometrician and Co-PI for the [Dynamic Learnings Maps](https://dynamiclearningmaps.org) assessments
  * PI for an [IES-funded](https://ies.ed.gov/funding/grantsearch/details.asp?ID=4546) project to develop software for diagnostic models
:::

:::{.column width="50%"}
:::{.center}

```{r}
#| label: profile-picture
#| out-width: 50%
#| fig-alt: |
#|   Profile picture for Jake Thompson.

include_graphics("figure/wjt-2022-hex.png")
```

:::{.small}
{{< iconify fa6-brands github >}} &nbsp; [@wjakethompson](https://www.gitub.com/wjakethompson)  
{{< iconify fa6-solid globe >}} &nbsp; [wjakethompson.com](https://wjakethompson.com)
:::
:::
:::
:::

## Acknowledgements

The research reported here was supported by the Institute of Education Sciences, U.S. Department of Education, through Grant [R305D210045](https://ies.ed.gov/funding/grantsearch/details.asp?ID=4546) to the University of Kansas. The opinions expressed are those of the authors and do not represent the views of the the Institute or the U.S. Department of Education. <br><br>

:::{.columns}
:::{.column width="15%"}
:::

:::{.column width="70%"}

```{r}
#| label: ies-logo
#| out-width: 100%
#| fig-align: center
#| fig-alt: |
#|   Logo for the Institute of Education Sciences.

include_graphics("figure/IES_InstituteOfEducationSciences_RGB.png")
```

:::

:::{.column width="15%"}
:::
:::

## Model fit for DCMs

* Absolute fit: How well does a model fit the data?
  * Model-level (global) fit  
  
  :::{.fragment .semi-fade-out fragment-index=1}
  * Item-level fit  
  :::
  
  :::{.fragment .semi-fade-out fragment-index=1}
  * Person-level fit
  :::

* Relative fit: How well does a model fit compared to another model?

* Different methods available depending on how the model was estimated (e.g., maximum likelihood, MCMC)

## Absolute fit with limited-information indices

* Categorical response data create sparse data matrices
  * 20 binary items: 2^20^ = `r prettyNum(2^20, big.mark = ",")` response patterns

* Limited-information indices use lower-order summaries of the contingencies tables ([Maydue-Olivares & Joe, 2005](https://doi.org/10.1198/016214504000002069))

* Most popular method for model fit in DCMs is the M~2~ ([Hansen et al., 2016](https://doi.org/10.1111/bmsp.12074); [Liu et al., 2016](https://doi.org/10.3102/1076998615621293))
  * *p*-value < .05 indicates poor model fit
  
## Bayesian absolute fit

* Not constrained to limited-information indices

* Posterior predictive model checks (PPMCs)
  * Generate new data sets from the posterior distribution
  * Calculate summary statistics for each generated data set
  * Compare the distribution to the observed value of the summary statistic

* In this study, we examine a PPMC of the the raw score distribution ([Park et al., 2015](https://doi.org/10.1504/IJQRE.2015.071738); [Thompson, 2019](https://doi.org/10.35542/osf.io/jzqs8))
  * 0.025 < *ppp* < 0.975 indicates good model fit

## Relative fit

* Information criteria such as the AIC ([Akaike, 1973](https://doi.org/10.1007/978-1-4612-1694-0_15)), BIC ([Schwarz, 1978](https://doi.org/10.1214/aos/1176344136)), or similar

* Compare the information criteria for each competing model
  * Index with the lowest value is preferred

* These methods are often inappropriate when using a Bayesian estimation process (e.g., MCMC; [Hollenbach & Montgomery, 2020](https://doi.org/10.4135/9781526486387))

## Bayesian relative fit

* Information criteria that are designed for Bayesian estimation methods

* Leave-one-out (LOO) cross validation with Pareto-smoothed importance sampling ([Vehtari et al., 2017](https://doi.org/10.1007/s11222-016-9696-4))
  * Asymptotically equivalent to the widely applicable information criterion (WAIC; [Watanabe, 2010](http://jmlr.org/papers/v11/watanabe10a.html)), but performs better in a wider range of conditions ([Gelman et al., 2014](https://doi.org/10.1007/s11222-013-9416-2)).
  
* As with more traditional methods, we compare the LOO for each competing model
  * Index with the lowest value is preferred
  * Can also calculate the standard error of the difference to help determine if the difference is meaningful

## State of the field

* Assessment of model fit is primarily limited to methods that rely on point estimates (e.g., M~2~, AIC, BIC)

* Research has not compared the efficacy of Bayesian measures of model fit to the more commonly used measures

* Recent software advances have made Bayesian estimation of DCMs more accessible to applied researchers
  * blatent ([Templin, 2023](https://doi.org/10.32614/CRAN.package.blatent))
  * measr ([Thompson, 2024](https://doi.org/10.32614/CRAN.package.measr))

## {.empty data-menu-title="measr" background-color="#023047" background-iframe="grid-worms/index.html"}

```{r}
#| label: measr-hex
#| out-width: 100%
#| fig-alt: |
#|   Hex logo for the measr R package.

include_graphics("figure/measr-hex.png")
```

## {.empty data-menu-title="Stan + measr" background-color="#FFFFFF"}

```{r}
#| label: stan-measr
#| out-width: 100%
#| fig-alt: |
#|   Stan logo and measr hex logo.

include_graphics("figure/combined-logos.png")
```

## The current study

:::{.columns}

:::{.column width="50%"}

* Simulation study to evaluate the efficacy of Bayesian measures of model fit

* Research questions:
  1. Do Bayesian measures provide an accurate assessment of model fit?
  2. How does the accuracy compare to more common methods for assessing model fit (e.g., M~2~)?

:::

:::{.column width="50%"}

:::{.center}
Preprint
:::

```{r}
#| label: preprint-qr-code
#| out-width: 70%
#| fig-alt: |
#|  QR code linking to https://doi.org/10.35542/osf.io/ytjq9

include_graphics(here("figure", "preprint-qr.png"))
```

:::

:::

## Simulation design

* 8 test design conditions
  * Attributes: 2 or 3
  * Items per attribute: 5 or 7
  * Sample size: 500 or 1,000

* Within each test design condition, we manipulated the data-generating and estimated model, for 4 total modeling conditions
  * Loglinear cognitive diagnostic model (LCDM; [Henson et al., 2009](https://doi.org/10.1007/s11336-008-9089-5))
  * Deterministic-input, noisy "and" gate (DINA; [de la Torre & Douglas, 2004](https://doi.org/10.1007/BF02295640))
  
* 32 conditions total, each repeated 50 times

## Expected outcomes

<br>
```{r}
library(gt)

crossing(generate = c("LCDM", "DINA"), estimate = c("LCDM", "DINA")) |> 
  mutate(abs = case_when(generate == estimate | estimate == "LCDM" ~ "No",
                         .default = "Yes"),
         rel = generate) |> 
  rename(`Generating model` = generate,
         `Estimated model` = estimate,
         `Absolute-fit flag` = abs,
         `Relative-fit preference` = rel) |> 
  gt() |> 
  cols_align(align = "center") |> 
  tab_options(table.font.names = "Open Sans",
              table.font.size = 30,
              column_labels.padding.horizontal = px(50)) |> 
  tab_style(style = cell_text(weight = "bold"),
            locations = cells_column_labels()) |> 
  tab_style(style = cell_borders(sides = "bottom", color = "black"),
            locations = cells_body(rows = 2)) |> 
  tab_style(style = cell_borders(sides = c("top", "bottom"),
                                 weight = px(3)),
            locations = cells_column_labels()) |> 
  tab_style(style = cell_borders(sides = "bottom", weight = px(3)),
            locations = cells_body(rows = 4))
```

## Results: Absolute fit

```{r}
#| label: read-results
#| include: false

results <- read_rds(here("data", "sim_results.rds")) |> 
  pluck("all_reps")
```

```{r}
#| label: calc-abs-fit
#| include: false

ppv <- function(x, truth, estimate) {
  x |> 
    select({{ truth }}, {{ estimate }}) |> 
    summarize(ppv = sum({{ truth }} & {{ estimate }}) / 
                sum({{ estimate }})) |> 
    pull(ppv)
}
npv <- function(x, truth, estimate) {
  x |> 
    select({{ truth }}, {{ estimate }}) |> 
    summarize(npv = sum(!{{ truth }} & !{{ estimate }}) /
                sum(!{{ estimate }})) |> 
    pull(npv)
}

abs_fit <- results |> 
  select(cond:rep, starts_with("lcdm_"), starts_with("dina_")) |> 
  pivot_longer(cols = -c(cond:rep),
               names_to = c("estimate", "metric"),
               names_pattern = c("([a-z]{4})_(.*)"),
               values_to = "value") |> 
  pivot_wider(names_from = metric, values_from = value) |> 
  relocate(estimate, .after = generate) |> 
  select(cond:rep, m2, m2_pval, ppmc_ppp) |> 
  mutate(true_flag = generate == "lcdm" & estimate == "dina",
         m2_flag = m2_pval < .05,
         ppmc_flag = ppmc_ppp < .05)

cond_abs_fit <- abs_fit |> 
  nest(flags = -c(attributes, items, sample_size)) |> 
  mutate(m2_ppv = map_dbl(flags,
                          \(x) ppv(x, truth = true_flag, estimate = m2_flag)),
         m2_npv = map_dbl(flags,
                          \(x) npv(x, truth = true_flag, estimate = m2_flag)),
         ppmc_ppv = map_dbl(flags,
                          \(x) ppv(x, truth = true_flag, estimate = ppmc_flag)),
         ppmc_npv = map_dbl(flags,
                          \(x) npv(x, truth = true_flag, estimate = ppmc_flag)))

overall_abs_fit <- abs_fit |> 
  select(cond:rep, m2, m2_pval, ppmc_ppp) |> 
  mutate(true_flag = generate == "lcdm" & estimate == "dina",
         m2_flag = m2_pval < .05,
         ppmc_flag = ppmc_ppp < .05) |> 
  nest(flags = everything()) |> 
  mutate(m2_ppv = map_dbl(flags,
                          \(x) ppv(x, truth = true_flag, estimate = m2_flag)),
         m2_npv = map_dbl(flags,
                          \(x) npv(x, truth = true_flag, estimate = m2_flag)),
         ppmc_ppv = map_dbl(flags,
                          \(x) ppv(x, truth = true_flag, estimate = ppmc_flag)),
         ppmc_npv = map_dbl(flags,
                          \(x) npv(x, truth = true_flag, estimate = ppmc_flag))) |> 
  select(-flags) |> 
  pivot_longer(cols = everything()) |> 
  deframe()
```

```{r}
#| label: fig-abs-cond
#| fig-asp: 0.42
#| fig-width: 8
#| fig-alt: |
#|   Four line graphs showing the positive and negative predictive values for
#|   each measure of absolute model fit, by test design condition.
#| out-width: "100%"

library(ggtext)

cond_abs_fit |> 
  select(-flags) |> 
  pivot_longer(cols = matches("_[pn]pv$")) |> 
  separate_wider_delim(name, delim = "_", names = c("fit_meas", "stat")) |> 
  mutate(attributes = factor(attributes, levels = c(2, 3),
                             labels = c("Two attributes", "Three attributes")),
         items = factor(items, levels = c(5, 7)),
         sample_size = factor(sample_size, levels = c(500, 1000),
                              labels = c("500", "1,000")),
         fit_meas = factor(fit_meas, levels = c("m2", "ppmc"),
                           labels = c("M<sub>2</sub>", "PPMC &chi;^2^")),
         stat = factor(stat, levels = c("ppv", "npv"),
                       labels = c("Positive<br>predictive<br>value",
                                  "Negative<br>predictive<br>value")),
         grp = paste0(attributes, items, fit_meas)) |> 
  ggplot() +
  facet_grid(cols = vars(attributes), rows = vars(stat)) +
  geom_point(aes(x = sample_size, y = value, shape = items, color = fit_meas)) +
  geom_line(aes(x = sample_size, y = value, linetype = items, color = fit_meas,
                group = grp)) +
  scale_color_manual(values = palette_measr[c(2, 1)]) +
  expand_limits(y = c(0.6, 1)) +
  labs(x = "Sample size", y = "Predictive value",
       shape = "Items per attribute", linetype = "Items per attribute",
       color = "Absolute-fit metric") +
  theme(strip.text = element_markdown(hjust = 0.5),
        legend.text = element_markdown()) +
  guides(color = guide_legend(order = 1))
```

## Results: Relative fit

```{r}
#| label: calc-rel-fit
#| include: false

both_fit <- abs_fit |> 
  select(cond:rep, ppmc_ppp) |> 
  pivot_wider(names_from = estimate, values_from = ppmc_ppp) |> 
  filter(lcdm > .05, dina > .05)

rel_fit <- results |> 
  semi_join(both_fit,
            join_by(cond, attributes, items, sample_size, generate, rep)) |> 
  select(cond:rep, starts_with("loo_"), starts_with("waic_")) |> 
  mutate(loo_decision = case_when(!loo_sig & generate == "dina" ~ "correct",
                                  !loo_sig & generate != "dina" ~ "wrong",
                                  loo_prefer == generate ~ "correct",
                                  loo_prefer != generate ~ "wrong"))

cond_rel_fit <- rel_fit |> 
  summarize(reps = n(),
            loo_correct = sum(loo_decision == "correct"),
            loo_wrong = sum(loo_decision == "wrong"),
            .by = c(attributes, items, sample_size, generate)) |> 
    mutate(pct_correct = loo_correct / reps)

overall_rel_fit <- rel_fit |> 
  summarize(loo_correct = sum(loo_decision == "correct"),
            loo_wrong = sum(loo_decision == "wrong")) |> 
  pivot_longer(cols = everything()) |> 
  deframe()
```

```{r}
#| label: fig-rel-cond
#| fig-asp: 0.42
#| fig-width: 8
#| fig-alt: |
#|   Two line graphs showing the proportion of replications where the
#|   data-generating model was correctly selected, by test design condition.
#| out-width: "100%"

cond_rel_fit |> 
  select(attributes:reps, pct_correct) |> 
  mutate(attributes = factor(attributes, levels = c(2, 3),
                             labels = c("Two attributes", "Three attributes")),
         items = factor(items, levels = c(5, 7)),
         sample_size = factor(sample_size, levels = c(500, 1000),
                              labels = c("500", "1,000")),
         generate = factor(generate, levels = c("dina", "lcdm"),
                           labels = c("DINA", "LCDM")),
         grp = paste0(attributes, items, generate)) |> 
  ggplot() +
  facet_grid(cols = vars(attributes)) +
  geom_point(aes(x = sample_size, y = pct_correct, shape = items, color = generate)) +
  geom_line(aes(x = sample_size, y = pct_correct, linetype = items, color = generate, group = grp)) +
  scale_y_percent() +
  scale_color_manual(values = palette_measr[c(2, 1)]) +
  expand_limits(y = c(0.6, 1)) +
  labs(x = "Sample size", y = "Repetitions With Correct Selection",
       color = "Data-generating model",
       shape = "Items per attribute", linetype = "Items per attribute") +
  theme(strip.text = element_markdown(hjust = 0.5),
        legend.text = element_markdown())
```

## Conclusions

* Bayesian methods performed as well or better for absolute fit

* Bayesian methods for relative fit performed as well or better than has been reported for other non-Bayesian information criteria

* Future directions
  * Larger, more complex test designs
  * Comparison to additional metrics (e.g., RMSEA, SRMSR)

## Learn more about measr

:::{.columns .v-center-container-slide}

:::{.column .center}
```{r}
#| label: measr-hex
#| out-width: 60%
#| fig-alt: |
#|   Hex logo for the measr R package.
```
:::

:::{.column}

:::{.large .spaced}
{{< fa globe >}} [measr documentation](https://measr.info)

{{< fa brands github >}} [wjakethompson/measr](https://github.com/wjakethompson/measr)
:::

:::

:::


# Thank you! {.thank-you data-menu-title="Get in touch" background-color="#023047"}

:::{.columns .v-center-container}

:::{.column .image width="65%"}

```{r}
#| label: measr-hex
#| out-width: 50%
#| fig-alt: |
#|   Hex logo for the measr R package.
```

:::

:::{.column width="35%"}

:::{.thank-you-subtitle}

:::{.small}

{{< iconify fa6-solid globe >}} \ [wjakethompson.com](https://wjakethompson.com)  
{{< iconify fa6-solid envelope >}} \ [wjakethompson@ku.edu](mailto:wjakethompson@ku.edu)  
{{< iconify fa6-brands linkedin >}} \ [in/wjakethompson](https://linkedin.com/in/wjakethompson)  
{{< iconify fa6-brands github >}} \ [@wjakethompson](https://github.com/wjakethompson)  
{{< iconify fa6-brands mastodon >}} \ [@wjakethompson@fosstodon.org](https://fosstodon.org/@wjakethompson)  
{{< iconify fa6-brands bluesky >}} \ [@wjakethompson.com](https://bsky.app/profile/wjakethompson.com)  
{{< iconify fa6-brands threads >}} \ [@wjakethompson](https://www.threads.net/@wjakethompson)  
{{< iconify fa6-brands x-twitter >}} \ [@wjakethompson](https://twitter.com/wjakethompson)  

:::

:::

:::

:::

# Appendix: Calculating the PPMC<br>raw score distribution &chi;^2^

## PPMC: Raw score by iteration {visibility="uncounted"}

```{r}
#| label: read-taylor

taylor_data <- read_rds(here("data", "taylor-data.rds"))
taylor_qmatrix <- read_rds(here("data", "taylor-qmatrix.rds"))

taylor_lcdm <- measr_dcm(
  data = taylor_data, qmatrix = taylor_qmatrix,
  resp_id = "album",
  type = "lcdm",
  method = "mcmc", backend = "rstan",
  warmup = 1000, iter = 1500,
  chains = 2, cores = 2,
  file = here("fits", "taylor-lcdm")
)

raw_score_needed <- !file.exists(here("data", "taylor-raw-scores.rds"))
```

```{r}
#| label: calc-raw-score
#| eval: !expr raw_score_needed

retain <- 500
keep_draws <- sample(seq_len(ndraws(as_draws(taylor_lcdm))),
                     size = retain)
taylor_results <- predict(taylor_lcdm, summary = FALSE, force = TRUE)

pi <- as_draws_df(taylor_lcdm) |> 
  merge_chains() |> 
  subset_draws(variable = "pi", draw = keep_draws) |> 
  as_tibble() |> 
  pivot_longer(starts_with("pi")) |> 
  separate_wider_regex(name,
                       patterns = c("pi\\[", item = "[0-9]*", ",", 
                                    class = "[0-9]*", "\\]")) |> 
  select(.draw, item, class, prob = value)

format_draws <- function(x, keep) {
  ret <- as_draws_df(x) |> 
    merge_chains() |> 
    subset_draws(draw = keep) |> 
    as_tibble() |> 
    select(.draw, starts_with("x")) |> 
    pivot_longer(cols = -.draw, names_to = "resp_id", values_to = "prob")

  return(ret)
}

all_draws <- vector(mode = "list", length = ncol(taylor_results$class_probabilities) - 1)
for (i in seq_len(ncol(taylor_results$class_probabilities))[-1]) {
  cur_name <- colnames(taylor_results$class_probabilities)[i]
  ret <- format_draws(taylor_results$class_probabilities[[i]], keep = keep_draws) |> 
    mutate(resp_id = str_replace(resp_id, "x\\[([0-9]*)\\]", "\\1"),
           resp_id = as.integer(resp_id)) |> 
    rename(!!cur_name := prob)
  
  all_draws[[i - 1]] <- ret
}

raw_scores <- all_draws |> 
  reduce(full_join, join_by(.draw, resp_id)) |> 
  pivot_longer(starts_with("[")) |> 
  group_by(.draw, resp_id) |> 
  mutate(class = 1:n()) |> 
  slice_sample(n = 1, weight_by = value) |> 
  ungroup() |> 
  select(.draw, resp_id, class) |> 
  mutate(.draw = as.integer(factor(.draw)),
         class = as.character(class)) |> 
  left_join(pi, join_by(.draw, class),
            relationship = "many-to-many") |> 
  mutate(rand = runif(n()),
         score = as.integer(rand <= prob)) |> 
  summarize(score = sum(score), .by = c(.draw, resp_id)) |> 
  count(.draw, score)

write_rds(raw_scores, here("data", "taylor-raw-scores.rds"))
```

```{r}
#| label: read-raw-score

raw_scores <- read_rds(here("data", "taylor-raw-scores.rds"))
```


:::{.columns}

:::{.column width="30%"}

* For each iteration, calculate the total number of respondents at each score point

:::

:::{.column width="70%"}

```{r score-dist}
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Scatter plot showing the number of respondents at each score point in each
#|   iteration.

p <- ggplot() +
  geom_point(data = raw_scores, aes(x = factor(score), y = n),
             position = position_jitter(height = 0, seed = 1213),
             alpha = 0.2, color = palette_measr[4]) +
  scale_y_comma() +
  labs(x = "Correct Responses", y = "Respondents")

p
```

:::

:::

## PPMC: Expected counts, by raw score {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* For each iteration, calculate the total number of respondents at each score point

* Calculate the expected number of respondents at each score point

:::

:::{.column width="70%"}

```{r exp-score}
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Scatter plot showing the number of respondents at each score point in each
#|   iteration with the average number of respondents overlayed.

exp_scores <- summarize(raw_scores, n = mean(n), .by = score)

p <- p +
  geom_point(data = exp_scores, aes(x = factor(score), y = n),
             color = palette_measr[1], shape = 18, size = 5) +
  geom_line(data = exp_scores, aes(x = factor(score), y = n),
            group = 1, color = palette_measr[1])

p
```

:::

:::

## PPMC: Observed counts {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* For each iteration, calculate the total number of respondents at each score point

* Calculate the expected number of respondents at each score point

* Calculate the <span style="color: #D7263D;">observed</span> number of respondents at each score point

:::

:::{.column width="70%"}

```{r obs-score}
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Scatter plot showing the number of respondents at each score point in each
#|   iteration with the average and observed number of respondents overlayed.

obs_scores <- taylor_data |> 
  pivot_longer(-album) |> 
  summarize(score = sum(value), .by = album) |> 
  count(score)

p <- p +
  geom_point(data = obs_scores, aes(x = factor(score), y = n),
             color = palette_measr[2], shape = 16, size = 5) +
  geom_line(data = obs_scores, aes(x = factor(score), y = n),
            color = palette_measr[2], group = 1)

p
```

:::

:::

## PPMC: &chi;^2^ summary statistic {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* For each replication, calculate a &chi;^2^~rep~ statistic

:::

:::{.column width="70%"}

```{r}
#| label: example-dist-compare
#| class-output: short
#| attr-output: 'style="max-height: 250px;"'

ppmc_chisq <- raw_scores |> 
  complete(.draw, score, fill = list(n = 0L)) |> 
  full_join(rename(exp_scores, exp = n), join_by(score)) |> 
  arrange(.draw, score) |> 
  mutate(piece = ((n - exp) ^ 2) / exp) |> 
  summarize(chisq = sum(piece), .by = .draw)

raw_scores |> 
  filter(.draw == 13) |> 
  full_join(exp_scores, by = "score") |> 
  rename(replication_n = n.x, expected = n.y)
```

$$
\chi^2_{rep} = \sum_{s=0}^S\frac{[n_s - E(n_s)]^2}{E(n_s)}
$$

```{r}
#| label: example-chisq

ppmc_chisq |> 
  filter(.draw == 13) |> 
  pull(chisq)
```

:::

:::

## PPMC: &chi;^2^ distribution {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* For each replication, calculate a &chi;^2^~rep~ statistic

* Create a distribution of the expected value of the &chi;^2^ statistic

:::

:::{.column width="70%"}

```{r chisq-dist}
#| label: chisq-dist
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Histogram of the chi-square values from each iteration.

p <- ggplot() +
  geom_histogram(data = ppmc_chisq, aes(x = chisq),
                 binwidth = 2, boundary = 0,
                 fill = palette_measr[1], color = palette_measr[4]) +
  labs(x = "&chi;<sup>2</sup><sub>rep</sub>", y = "Replications") +
  theme(axis.title.x = ggtext::element_markdown(family = "sans"))

p
```

:::

:::

## PPMC: &chi;^2^ observed value {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* For each replication, calculate a &chi;^2^~rep~ statistic

* Create a distribution of the expected value of the &chi;^2^ statistic

* Calculate the &chi;^2^ value comparing the <span style="color: #D7263D;">observed</span> data to the expectation

:::

:::{.column width="70%"}

```{r}
#| label: chisq-obs
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Histogram of the chi-square values from each iteration with a dashed
#|   vertical line indicating the value from the observed data.

obs_chisq <- obs_scores |> 
  full_join(rename(exp_scores, exp = n), join_by(score)) |> 
  replace_na(list(n = 0L)) |> 
  arrange(score) |> 
  mutate(piece = ((n - exp) ^ 2) / exp) |> 
  summarize(chisq = sum(piece))

p <- p +
  geom_vline(xintercept = obs_chisq$chisq,
             linetype = "dashed", color = palette_measr[2],
             linewidth = 1)

p
```

:::

:::

## PPMC: Posterior predictive *p*-value {visibility="uncounted"}

:::{.columns}

:::{.column width="30%"}

* Calculate the proportion of &chi;^2^~rep~ draws greater than our observed value

* Flag if the observed value is outside a predefined boundary (e.g., .025 < *ppp* < 0.975)

* In our example *ppp* = `r round(mean(ppmc_chisq$chisq > obs_chisq$chisq), digits = 3)`

:::

:::{.column width="70%"}

```{r}
#| label: chisq-obs
#| out-width: 100%
#| out-height: 50%
#| fig-alt: |
#|   Histogram of the chi-square values from each iteration with a dashed
#|   vertical line indicating the value from the observed data.
```

:::

:::
